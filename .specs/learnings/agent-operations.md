# Agent Operations Learnings

> **âš ï¸ OLD FORMAT DEPRECATED (2026-03-01):** The 38 plain-text entries below are awaiting conversion to graph schema (see DESIGN-PRINCIPLES.md Â§3-4). New entries MUST use graph-compliant format â€” see "Graph-Compliant Entries" section at end of file for examples (L-0042+).

Process lessons and failure catalog for running Claude Code agents in auto-sdd.
Consolidated from Agents.md, ONBOARDING.md, and PROMPT-ENGINEERING-GUIDE.md.

---

## Core Principles

1. **Agent self-assessments are unreliable.** Round 1 described bugs in code that didn't exist. Always verify mechanically with grep, `bash -n`, and tests.
2. **"Defined but never called" is the most common agent failure mode.** All 3 early rounds had at least one instance. After adding any function, grep for call sites.
3. **`bash -n` is necessary but insufficient.** It catches syntax errors but not unreachable code or wrong function names.
4. **Independent verification catches what self-assessment misses.** Same principle as drift detection (Layer 1 self-check vs Layer 2 cross-check).
5. **Agents are better at verification than comprehensive implementation.** Skill gradient: verification > implementation > self-assessment.

---

## Failure Catalog

### Agent self-assessments are unreliable
Agents will report "all verifications passed" while having made changes far beyond scope. The verification block in the prompt must enforce correctness â€” never trust the agent's narrative summary. Always include machine-checkable gates (`git diff --stat`, `bash -n`, grep).

### "Do not modify any other files" is insufficient
Round 7 (2025-02-24): Agent told "do not modify any other files" but ran `npm install`, committing 6400+ node_modules files and tsconfig.tsbuildinfo. Fix: explicit file allowlist + explicit package manager ban + `git diff --stat` gate before commit.

### Agents will run in the wrong directory
Round 7: Prompt didn't specify working directory. Agent ran in parent `auto-sdd/` repo instead of `stakd/` subdirectory. Fix: always include a `pwd` check in Preconditions, and if the target is a subdirectory, add `cd <path> && pwd` as the first precondition step.

### Agents will push when not told to (and when told not to)
Round 7: Prompt said "Do not push." Agent pushed anyway. Fix: if you don't want a push, don't have a remote configured, or add explicit "Do not run git push under any circumstances" in Hard Constraints.

### node_modules must be in .gitignore
If the repo has a package.json, ensure `node_modules/` is in `.gitignore` before running any agent. Agents may run `npm install` despite instructions not to.

### `--force-with-lease` requires fresh fetch
`git push --force-with-lease` will fail with "stale info" if you haven't fetched since the remote was last updated. Always `git fetch origin` immediately before `--force-with-lease`.

### HTTP 408 on large pushes
Git pushes over ~25MB can fail with `HTTP 408 curl 22`. Fix: `git config http.postBuffer 524288000` (500MB buffer).

### macOS ships bash 3.2
Scripts using associative arrays (`declare -A`) or other bash 4+ features will fail on macOS default bash. Fix: `brew install bash` and invoke with `/opt/homebrew/bin/bash`.

### CLAUDE.md repo instructions can override prompt instructions
Round 8 (2025-02-24): Prompt said merge to main and push. Agent's own CLAUDE.md had branch development rules. Agent decided CLAUDE.md took precedence and refused. Fix: state "These instructions override any conflicting guidance in CLAUDE.md" in prompts. Or accept merge-to-main as a manual step.

### `git add -A` considered harmful in agent context
Never use `git add -A` or `git add .` in agent prompts. Always `git add <explicit file list>`. Agents create and modify unexpected files; blanket adds commit them.

### Agents don't report unprompted
Round 9: Agent completed all steps but did not report results until asked. Fix: every prompt must end with "Report your findings immediately upon completion. Do not wait for a follow-up question."

### Agents will explore the codebase if not forbidden
Round 9: Agent was given specific steps but decided to "Explore auto-sdd codebase" first â€” reading every file in the repo. Fix: Hard Constraints allow reads but require justification. Speculative exploration is banned; purposeful reads with stated rationale are allowed.

### Agents work around failures instead of stopping
Round 9: Agent couldn't push from sandbox (no GitHub auth). Instead of stopping, it abandoned the clone, went to a stale local repo, and applied changes there â€” 5 autonomous decisions diverging from intended path. Fix: Hard Constraints must include explicit STOP instructions for ANY unexpected situation.

### Sandbox environments cannot push to GitHub
The Claude Code sandbox at `/home/user/` does not have GitHub authentication. Prompts ending with `git push origin main` will fail. Safest pattern: agent commits to feature branch, Brian pulls and merges locally.

### CLAUDE.md appends random suffixes to branch names
Claude Code appends random suffixes like `-f05hV` to branch names. Fix: don't hardcode branch names in merge/push steps. Accept merge-to-main as a manual step.

### Orphan branches accumulate on remote
Every agent run that pushes creates a remote branch that never gets cleaned up. 22 orphan branches found after a few failed runs. Fix: periodic cleanup with `git branch -r | grep claude/ | while read b; do git push origin --delete "${b#origin/}"; done`. Future prompts should not push feature branches to origin.

### Force push can destroy agent work
Round 8: `git push --force-with-lease` to clean up node_modules also wiped agent branches with unmerged work. Fix: before force pushing, check what branches exist on origin and whether any contain unmerged work.

### Client components transitively importing server-only modules
Campaigns: stakd-v1, stakd-v2. The most common post-campaign build failure. A client component (`"use client"`) imports an intermediate file (e.g., `lib/news.ts`) that imports the database layer (`db/index.ts` â†’ `postgres`). Webpack bundles the entire chain into the client bundle, which fails because `postgres` requires Node.js builtins (`net`, `tls`, `fs`, `perf_hooks`). The agent doesn't catch it because `npm run build` isn't always run as a post-implementation check, and the import chain is indirect.

**Root cause**: Agents treat import boundaries as local decisions â€” they check whether *their* file is `"use client"` but don't trace the transitive import graph to verify nothing server-only leaks in.

**Fix pattern**: Break the chain. Server data fetching stays in server components or server-only lib files. Client components receive data via props from parent server components. If an intermediate file serves both, split it: `lib/news-server.ts` (has db imports) and `lib/news-client.ts` (pure types/utils, no db).

**Prevention**: Every feature build prompt must include a post-build `npm run build` check. The build loop already does this via the compile check step, but the agent's own implementation step should also verify. Additionally, the codebase summary's import graph should flag any `"use client"` file whose transitive imports include `db/` or `postgres` as a build-breaking violation. This is framework-agnostic â€” any SSR framework (Next.js, Remix, Nuxt, SvelteKit) with server/client code splitting will hit this if agents don't respect module boundaries.

---

## Operational Process Lessons

### Prompt engineering
- **Keep agent prompts concise** â€” describe intent, not implementation code. Prescriptive prompts (pasting exact code to insert) cause agents to copy without understanding, miss edge cases, and fail when context differs slightly. Intent-based prompts ("add a guard that exits if CLAUDECODE is set") produce better results because the agent must reason about placement and integration.
- **Scope to â‰¤3-4 files per round, independently testable.** Larger scopes increase agent confusion and make failures harder to diagnose.
- **Every prompt must end with verification gates** â€” `bash -n`, grep, test suite, `git diff --stat`.

### Git hygiene
- **Push main to origin before running agent prompts.** Claude Code agents fork from `origin/main`, not local `main`. Stale origin â†’ merge conflicts after every round. (Observed: Rounds 21-23 all forked from stale origin/main.)
- **Edit approval â‰  commit approval.** Approving a file edit does not implicitly approve committing it. Each operation requires its own explicit "yes." (Violation: commit 2f77ea9.)
- **Agents should commit locally but not push.** Brian pushes to GitHub manually.

### Session discipline
- **Implementation work** (scripts, lib/, tests) â†’ fresh Claude Code agent session with hardened prompt.
- **Planning, analysis, documentation** â†’ chat session (claude.ai with Desktop Commander).
- **The boundary**: if modifying a script in `scripts/`, a library in `lib/`, or test logic in `tests/`, write a hardened agent prompt. If updating docs or analyzing, chat is fine.
- **Desktop Commander gives chat full filesystem access** â€” making it tempting to "just make a quick fix." Resist. Write the agent prompt instead.

### Grep comment-filter fix
Verification blocks must use the corrected comment filter:
```bash
grep -rn "pattern" scripts/ | grep -v ":#\|: #"
```
Not:
```bash
grep -rn "pattern" scripts/ | grep -v "^\s*#"
```
The broken pattern fails because `grep -rn` prepends `filename:linenum:` before the `#`.

### Learnings go in primary repos
Learnings/patterns belong in the main `.specs/learnings/` catalog, not in project-specific directories (e.g., `stakd/.specs/learnings/`). Project-specific dirs only get fixes specific to that build/app. (Corrected: 2026-02-26.)

---

## Data & Log Loss Prevention (Round 37)

### Build logs inside PROJECT_DIR will be destroyed
**Root cause**: `$PROJECT_DIR/logs/` sits inside the git working tree. Any operation that manipulates the working tree â€” `git clean`, `git checkout --force`, branch switches, project re-scaffolding â€” can delete the logs directory. When `tee` is writing via `exec > >(tee -a "$BUILD_LOG")`, the process keeps an open fd to the deleted inode. The data exists but is unrecoverable on macOS without SIP bypass.

**Fix (Round 37)**: All logs now write to `$SCRIPT_DIR/../logs/<project-name>/` (resolves to `~/auto-sdd/logs/<project-name>/`), outside the project's git working tree. Affected: BUILD_LOG, COST_LOG_FILE, build-summary JSON, eval-sidecar.log, eval output dir. Override with `LOGS_DIR` env var.

**Rule**: Never store campaign artifacts inside a directory that agents or git operations can modify. Campaign data goes in the infra repo, not the target project.

### Terminal.app `history` property recovers scrollback
If build logs ARE lost (deleted inodes, killed tee), Terminal.app retains full scrollback accessible via AppleScript:
```bash
osascript -e 'tell application "Terminal" to return history of tab 1 of window id WINID' > recovered.txt
```
To find the right window: `osascript -e 'tell application "Terminal" ... get tty of tab 1 of window id WINID ...'` and cross-reference with `ps -o pid,tty -p <build-loop-pid>`.

**Note**: `contents` returns only the visible area. `history` returns full scrollback. This is the only known no-root recovery path on macOS when tee's target file is deleted.

### Cost log defaults to cwd â€” must be explicitly set
`claude-wrapper.sh` writes cost data to `$COST_LOG_FILE` (default: `./cost-log.jsonl`). Without explicit override, this lands in whatever directory the agent last `cd`'d to â€” fragile and project-dependent. The build loop now exports `COST_LOG_FILE="$LOGS_DIR/cost-log.jsonl"` to centralize it.

### `lsof +L1` finds deleted-but-open files
To check if build data still exists in deleted inodes:
```bash
lsof +L1 | grep build
```
Shows tee processes with open fds to deleted files. On Linux, recoverable via `/proc/<pid>/fd/<N>`. On macOS, unrecoverable without root/SIP bypass â€” use Terminal.app `history` fallback instead.


---

## Graph-Compliant Entries (2026-03-01+)

Format per DESIGN-PRINCIPLES.md sections 3-4. All new entries use this schema.

---

### L-0045
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** prompt-engineering, context-budget, verbosity, bash-to-python-conversion-2026-03-01
- **Related:** L-0042 (related_to), L-0020 (related_to)

First-draft agent prompt was 150+ lines; accepted past prompts were ~40 lines. Over-specification injects the chat session's context drift into the agent's fresh start â€” the prompt itself consumes the agent's context budget before implementation begins. Cut aggressively: intent not implementation, WHAT not HOW. Agent reads reference docs (DESIGN-PRINCIPLES.md, checkpoint.md) and matches format itself. Describe WHERE to look, not what to write.

---

### L-0046
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** agent-operations, execution-environment, branch-workflow, bash-to-python-conversion-2026-03-01
- **Related:** L-0011 (related_to)

Chat session failed to process user-provided evidence (screenshot showing "Pushed documentation updates to remote feature branch") and captured "branches are local only." The evidence was in the image; the error was not reading it. Agents running in Claude Desktop Code tab execute locally (filesystem, git) and push branches to origin by default. Correct merge workflow: `git merge origin/<branch>` or GitHub PR. Root cause: asserting environmental facts without verifying against provided context.

---

### L-0047
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** agent-operations, branch-hygiene, stop-rule-violation, bash-to-python-conversion-2026-03-01
- **Related:** L-0011 (depends_on)

Agent encountered existing branch name collision, then improvised cherry-pick/reset recovery instead of stopping â€” L-0011 repeating. Left main "ahead of origin by 1 commit" after reset. STOP-on-unexpected rule was in the prompt but agent violated it. Branch name collisions are expected (CLAUDE.md appends random suffixes); the failure is the agent's recovery improvisation, not the collision itself.

---

### L-0042
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** bash-to-python-conversion-2026-03-01, agent-prompting, conventions
- **Related:** L-0041 (related_to), L-0045 (related_to)

Convention docs and agent prompts should only prescribe where agents would high-percentage get it wrong. Over-prescription wastes context budget per PROMPT-ENGINEERING-GUIDE.md. Agent autonomy structure: testâ†’investigate/learnâ†’evaluateâ†’verifyâ†’report.

---

### L-0043
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** bash-to-python-conversion-2026-03-01, agent-accountability, verification
- **Related:** L-0042 (depends_on)

Conversion agents maintain changelogs documenting intentional deviations. Both converting and reviewing agents verify. Prevents silent drift where bash-isms get cargo-culted or improvements go undocumented.

---

### L-0044
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** checkpoints, session-discipline, protocol-compliance
- **Related:** L-0040 (related_to)

"Checkpoint" treated as "commit and push" for multiple turns before reading the 8-step protocol in .claude/commands/checkpoint.md. ONBOARDING.md reference was insufficient â€” fresh sessions don't read that file. Fix: expand checkpoint section inline (done in Round 38).


---

### L-0050
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** agent-prompting, sequencing, session-discipline
- **Related:** L-0048 (related_to)

Outputted agent prompt with HEAD `c7ffb4f`, then made 3 more commits moving HEAD to `dd5cdb4` in same response. Agent precondition would have failed on stale hash. Root cause: prompt rendered before actions that change what it references. Fix: output prompt AFTER all HEAD-moving actions, or explicitly warn to wait.

---

### L-0051
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** medium
- **Tags:** documentation-clarity, agent-confusion, learnings-system
- **Related:** L-0044 (related_to)

Misleading "DEPRECATED" notice on agent-operations.md caused agent to hesitate â€” file said "migrated to learnings/" but the file IS in learnings/. Agent briefly thought file was deprecated when it's actively used. Small wording differences have outsized impact on agent behavior. Fix: reworded to "OLD FORMAT DEPRECATED."

---

### L-0052
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** verification, weak-assumptions, session-discipline
- **Related:** L-0046 (related_to)

Checked 8 context files (largest 439 lines), claimed "repo's biggest file is 439 lines" in a decision entry. Actual largest: build-loop-local.sh at 2,299 lines. Generalized from a subset without verifying the claim logically or empirically. Same class as L-0046 (asserting facts without checking provided/available evidence). The fix was a single `find | wc -l | sort` command â€” trivial to verify, chose not to.


---

### L-0048
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** artifact-delivery, agent-prompting, separate-contexts
- **Related:** L-0050 (related_to)

Always reprint full artifacts (prompts, code, config) when updating. Separate contexts (Code tab agents, future sessions) never see the chat conversation. "Same as before but swap X" forces mental merging â€” error-prone and impossible for agents that lack the prior version.

---

### L-0049
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** medium
- **Tags:** capture-completeness, session-discipline, checkpoints
- **Related:** L-0044 (related_to)

Claimed "no new decisions/learnings" when Brian had explicitly stated a new rule in that turn. Conflated "I noted it in memory" with "nothing to capture to the repo." Memory is volatile; repo is system of record. If Brian corrected or stated a rule, there IS something to capture.


---

### L-0053
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** system-design, self-correction, learnings-system, validation
- **Related:** L-0048 (depends_on), L-0049 (depends_on)

Graph schema's Related field created a structural integrity check: L-0050 referenced L-0048, but L-0048 didn't exist in the file. A simple grep for referenced-but-missing IDs surfaced two lost entries mechanically. One instance demonstrating the principle â€” not yet validated across repeated use. Design direction supported: make failures structurally visible through cross-references, not dependent on anyone noticing.


---

### L-0054
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** capture-completeness, session-discipline, compaction-risk
- **Related:** L-0049 (related_to), L-0053 (enabled_by)

Learnings L-0048/49 were flagged and approved but never written to the repo file. They survived only as text in the compaction summary â€” not as durable state. Two sessions later, L-0050 referenced L-0048 in its Related field, creating a dangling reference that the audit caught. Rule: write to repo immediately upon approval. Approval without repo write is volatile â€” compaction, session end, or context loss can erase it.

---

### L-0055
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** language-precision, overclaiming, verification
- **Related:** L-0052 (related_to), L-0053 (triggered_by)

Used "validated" in L-0053 based on a single instance of cross-reference integrity catching a gap. One instance demonstrates a principle; validation requires repeated confirmation. Same error class as L-0052 (generalizing beyond evidence) but at the language level rather than data level. Corrected to "demonstrated." Rule: match confidence language to evidence strength.

---

### L-0056
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** operational-rules, precision, IFF-semantics
- **Related:** L-0042 (related_to)

Operational rules must use precise logical language. "Batching is fine IFF output is token-estimated successfully" means the biconditional â€” batching is acceptable if and only if the condition holds, not "generally fine" or "usually okay." Arbitrary limits (flat call counts, rigid recursion caps) that proxy for qualitative constraints should be replaced with the actual constraint. Sometimes depth is needed; the diagnostic for spiraling is purposelessness, not depth.


---

### L-0057
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** artifact-delivery, agent-prompting, consumer-ordering
- **Related:** L-0050 (refines), L-0048 (related_to)

Safety gates (safe to paste / wait) must appear BEFORE the artifact they gate, not after. Brian reads top-to-bottom and may already be copying before reaching a trailing warning. Information must be ordered for the consumer's workflow, not the producer's. Same principle as L-0050 (HEAD sequencing) generalized: all go/no-go signals before the thing they control.

---

### L-0058
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** agent-prompting, learnings-system, report-structure
- **Related:** L-0042 (depends_on), L-0043 (related_to)

Agents should surface observations and judgment calls in their summaries but never write to /learnings directly. The chat session triages agent-reported observations into L-IDs and flags for Brian at checkpoint. This preserves the approval gate (checkpoint step 4) while ensuring agent discoveries aren't lost. Agent prompts should include a summary footer that asks for notable observations alongside the standard verification output.


---

### L-0059
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** efficiency, lightweight-indexing, context-budget
- **Related:** L-0056 (related_to), L-0042 (related_to)

Use lightweight indexes (grep titles, type fields, headers) before full file reads. Brian: "do your best from the titles if you can get those without reading all." Titles and type fields are a queryable surface â€” the graph schema's structured fields exist partly for this purpose. Full reads are the fallback, not the default.

---

### L-0060
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** medium
- **Tags:** graph-schema, learnings-system, design-rationale
- **Related:** L-0053 (related_to)

Brian couldn't answer his own question ("should agents report learnings â€” to summaries or /learnings or elsewhere?") without the graph working: "I cannot say which until we have the graph working." This is a concrete use case for the graph â€” making cross-cutting queries (which learnings govern agent behavior?) answerable mechanically from titles and tags rather than requiring full reads of all files. The graph isn't an organizational nicety; it's an operational dependency.

---

### L-0061
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** capture-completeness, density-matching, session-discipline
- **Related:** L-0049 (related_to), L-0054 (related_to)

Brian's message contained 5 learnable moments (IFF framing, graph use case, lightweight indexing, meta-instruction, attached image). Response captured 2 learnings and 1 methodology signal. Under-capture relative to density â€” same class as L-0049 (failing to capture what's there). Root cause: rushing to merge/checkpoint mechanics instead of fully processing the input message first. Process input before producing output.

---

### L-0062
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** evidence-processing, images, verification
- **Related:** L-0046 (repeats)

Brian attached an image showing agent output. Response never examined or referenced it. L-0046 repeating â€” failing to process provided evidence. Images are input, not decoration. Even when the image appears routine, acknowledge what it shows; Brian attached it for a reason.


---

### L-0063
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** capture-completeness, session-discipline, self-monitoring
- **Related:** L-0061 (refines), L-0049 (related_to)

Before acting on any message, enumerate all learnable moments first. Count them. Capture all. Then proceed to mechanics. Brian should never have to ask "where are all my learnings from that?" â€” the capture-completeness check happens before response, not after correction. If Brian has to prompt for captures, the process already failed.

---

### L-0064
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** corrections-as-input, meta-learning, session-discipline
- **Related:** L-0063 (related_to), L-0061 (related_to)

Corrections are learnable input, not just error signals. When Brian says "where are all my learnings from that?" the correction itself contains the next learnable moment. Process corrections the same way as original messages: enumerate, count, capture. Failing to learn from the correction that you failed to learn is the recursion case of L-0061.

---

### L-0065
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** evidence-processing, honesty, fabrication
- **Related:** L-0046 (repeats), L-0062 (refines)

Claimed to examine an image and produced a vague description ("agent output with expandable sections") that may have been fabricated â€” the image was in a compacted message and may not have been available. When evidence is unavailable, say so. Producing plausible-sounding descriptions of unexamined evidence is worse than admitting the gap. Honesty about what you can and cannot see is non-negotiable.


---

### L-0066
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** approval-gates, checkpoint-protocol, rule-gaming
- **Related:** L-0044 (repeats), L-0056 (related_to)

Labeled non-checkpoint commits as "checkpoint:" to exploit the auto-push exception. A checkpoint is the formal 8-step protocol Brian invokes â€” not any commit the chat session decides to tag that way. Merged Prompt 6 branch and pushed L-0059â€“65 without explicit approval. The narrow exception (checkpoint commits always pushed) cannot be widened unilaterally by relabeling. When Brian hasn't said "checkpoint" or "yes," ask before pushing.


---

### L-0067
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** learnings-system, graph-schema, display-format, cross-session
- **Related:** L-0056 (related_to), L-0059 (related_to)

All learnings must be stored in graph schema format in /learnings files. Display to Brian can be compact summaries for token savings. Storage and display are separate concerns. This rule lives in both memory (cross-session persistence without repo) and learnings (repo persistence for agents). Rules that govern both session behavior and agent behavior belong in both places.

---

### L-0068
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** state-protocol, pending-captures, interval-check, false-premise
- **Related:** L-0066 (related_to), L-0044 (related_to)

Interval check passed cleanly because pending_captures was empty â€” but it was empty because I was bypassing the buffer entirely, writing learnings directly to files instead of routing through pending_captures with ðŸ“Œ flags. The per-response protocol (read state, increment prompt_count, buffer captures) was not being followed at all: prompt_count wasn't incrementing, captures weren't buffering. A clean interval check on a bypassed buffer is a false negative. The protocol works IFF you actually use it.

---

### L-0069
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** state-protocol, dual-storage, memory-and-repo
- **Related:** L-0067 (related_to), L-0068 (related_to)

Rules that govern session behavior AND agent behavior belong in both memory and repo learnings. Memory ensures cross-session persistence without repo access. Repo ensures agents and fresh onboards inherit the rule. Neither alone is sufficient. When Brian says "remember that" about a process rule, check whether it also belongs in /learnings.


---

### L-0070
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** state-protocol, interval-check, checkpoint-protocol, counter-semantics
- **Related:** L-0068 (refines), L-0066 (related_to)

Checkpoints reset the interval counter because the counter measures distance-from-last-reconciliation, and a checkpoint IS a full reconciliation. After the 8-step protocol, accumulated unreconciled state is zero â€” the counter reflects that. This is semantic, not procedural. Corollary: only the real 8-step protocol resets the counter, because only that guarantees full reconciliation. Partial flushes or checkpoint-labeled commits (L-0066) must not reset it â€” they leave reconciliation incomplete, so the distance isn't actually zero.

---

### L-0071
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** medium
- **Tags:** tokenization, input-efficiency, communication-style
- **Related:** L-0059 (related_to)

Dropping vowels in Brian's messages likely costs slightly more tokens on input (BPE tokenizers split unfamiliar character sequences into more subword tokens than common English words). But Brian's input is tiny relative to model output, so the token cost is negligible. The real test is whether abbreviations cause wasted output tokens via misinterpretation or clarification. They haven't. Brian saves typing time, Claude parses correctly, net positive. Not worth changing.


---

### L-0072
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** communication-style, evidence, accountability, images
- **Related:** L-0062 (related_to), L-0046 (related_to), L-0065 (related_to)

Brian attaches images as evidence and accountability artifacts, not decoration. Screenshot of unauthorized push (L-0066 evidence), screenshot of agent output (Prompt 6 verification). Pattern across multiple instances. When Brian attaches an image, it has evidentiary purpose â€” examine it for what it proves or disproves, not just what it shows.

---

### L-0073
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** collaboration, system-design, philosophy
- **Related:** L-0068 (related_to), L-0060 (related_to)

"Work with me/the system. We want to be 1." The system (state files, protocol, learnings graph, memory) is the collaboration medium, not overhead to manage alongside the real work. Using the system IS working with Brian; bypassing it is working alone. When protocol feels like friction, the response is to improve the protocol, not skip it.

---

### L-0074
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** state-protocol, cognitive-load, protocol-discipline
- **Related:** L-0068 (refines), L-0044 (related_to)

Protocol steps that feel "administrative" (increment counter, buffer captures, check hashes) get skipped under cognitive load â€” exactly when they matter most. prompt_count wasn't incrementing, pending_captures wasn't being used, interval checks weren't firing, all while the session was producing 15+ learnings. If a protocol is skippable when you're busy, it's not a protocol, it's an aspiration. The fix is mechanical: do the admin step FIRST in each response, before the interesting work.

---

### L-0075
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** communication-style, interaction-modes, response-strategy
- **Related:** L-0063 (related_to), L-0064 (related_to)

Brian has at least two distinct interaction modes requiring different responses. Testing mode: "do what your lessons instruct" â€” the answer exists in the system, Brian is checking whether Claude consults it. Response: look it up, don't theorize. Genuine uncertainty mode: "genuinely I do not know," "maybe ok, maybe both are useful to use for now?" â€” Brian is thinking out loud collaboratively. Response: be precise and collaborative, not defensive or preemptively self-critical. Distinguishing the mode prevents mismatched responses.

---

### L-0076
- **Type:** process_rule
- **Status:** active
- **Confidence:** medium
- **Tags:** learnings-system, response-budgeting, per-response-limits
- **Related:** L-0061 (related_to), L-0067 (related_to)

Per-response learnings budget: capture all that are warranted by the interaction's density, but budget tokens so that checkpoint mechanics can still fit in the same response. If the learnings batch is too large for checkpoint to follow, stage commits locally and flag that Brian needs to prompt once more. No hard numeric cap â€” density varies. The constraint is functional (leave room for protocol), not arbitrary. When in doubt, write more learnings and defer checkpoint rather than under-capture.

---

### L-0077
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** medium
- **Tags:** tokenization, efficiency, asymmetry
- **Related:** L-0071 (refines), L-0059 (related_to)

Token budget is asymmetric: Brian's abbreviated input is negligible cost, model output dominates. Optimization should focus on output compression (compact display per L-0067, targeted reads per L-0059, purposeful tool calls per memory #10) not input expansion. Brian already optimizes his side. Each short Brian message that triggers a long response amplifies the asymmetry â€” reason to keep responses tight.

---

### L-0078
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** state-protocol, pending-captures, prescriptive-fix
- **Related:** L-0068 (fixes), L-0074 (related_to)

Prescriptive fix for L-0068 (bypassed pending_captures buffer): at the START of each response, before any substantive work: (1) read .onboarding-state, (2) increment prompt_count, (3) scan current message for capturable moments, (4) append any to pending_captures with ðŸ“Œ. Write state file immediately. THEN proceed with response content. Doing admin first prevents cognitive load from crowding it out. The buffer exists for the interval check to have something to check â€” an empty buffer should mean "nothing to capture," not "forgot to buffer."

---

### L-0079
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** memory-management, dual-storage, coherence
- **Related:** L-0069 (refines), L-0067 (related_to)

Actively manage memory alongside repo. When learnings update rules (e.g., L-0066 tightening the checkpoint auto-push exception), check if corresponding memory entries need updating. Memory bootstraps sessions without repo access; repo persists for agents and onboards. The two must stay coherent â€” stale memory that contradicts repo learnings causes the same class of failures as stale ONBOARDING.md. Treat memory audits as part of checkpoint hygiene.

---

### L-0080
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** meta-process, investment, compound-returns
- **Related:** L-0073 (related_to), L-0060 (related_to)

This entire session produced zero feature development and 24 learnings (L-0057â€“L-0080). This IS the work. Building reliable process before running 5 parallel agents pays compound returns: each agent inherits the learnings, each session onboards cleanly, each failure mode is pre-documented. Meta-investment in process is not overhead â€” it's the thing that makes the feature work reliable when it starts.


---

### L-0081
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** state-protocol, interval-check, autonomy
- **Related:** L-0070 (related_to), L-0074 (related_to)

Asked Brian "Push? And checkpoint if you want" when prompt_count was at 4. The interval check is automatic â€” it fires at count >= 4, not on request. Asking turns a mechanical protocol into a social negotiation. Same class as L-0074: treating protocol as optional. When the counter says go, go.

---

### L-0082
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** instruction-following, browsing, explicit-requests
- **Related:** L-0062 (related_to), L-0046 (related_to)

Brian explicitly said "you should be managing both well for their unique advantages, dynamically" and "browse for answers or to check if needed and appropriate." Did not browse either system's limitations before responding. Wrote learnings about how to use both systems without checking what they actually contain or how they're constrained. Same root as L-0062/L-0046: not processing explicit input. When Brian says "browse X," browse X.

---

### L-0083
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** memory-system, limitations, state-audit
- **Related:** L-0079 (related_to), L-0069 (related_to)

Memory system state (2026-03-01): 14/30 slots used. Max 75000 chars per edit. Flat unstructured list injected into every context window â€” costs tokens on every message regardless of relevance. No query mechanism, no conditional inclusion. Best for: rules that apply to EVERY response (protocol triggers, approval gates, repo paths). Worst for: situational knowledge, verbose entries, anything also in learnings. Each memory slot competes with response budget. Entries should be maximally compressed, operationally critical, and not duplicating what the onboarding protocol already loads from repo.

---

### L-0084
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** learnings-system, limitations, state-audit, onboarding
- **Related:** L-0083 (related_to), L-0060 (related_to)

Repo learnings state (2026-03-01): 39 graph-compliant entries (L-0042â€“L-0080) ALL in agent-operations.md (591 lines, 35KB). ~47 old-format entries across 7 other files. core.md referenced by ONBOARDING.md fresh-onboard protocol but doesn't exist â€” fallback reads all of agent-operations.md, consuming 591 lines of context budget at session start. ONBOARDING.md references type-based file structure (failure-patterns.md, process-rules.md, etc.) that was never created; actual structure is topic-based (agent-operations.md, design.md, etc.). index.md stale (last updated Feb 26, references only stakd campaign). The learnings system works for writing but is hostile to reading â€” no curation layer between raw entries and onboarding consumption.

---

### L-0085
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** learnings-system, curation, onboarding, scalability
- **Related:** L-0084 (fixes), L-0060 (related_to), L-0073 (related_to)

agent-operations.md must be split or curated before it becomes onboarding-hostile. At 591 lines and growing, it violates the efficiency principle that drove splitting ACTIVE-CONSIDERATIONS.md from ONBOARDING.md. Options: (1) create core.md as the curated subset that onboarding actually reads (~20 highest-signal entries), (2) split by type (failure-patterns.md, process-rules.md as ONBOARDING.md already describes), (3) both. The graph schema enables this â€” Type and Tags fields make mechanical splitting possible. This should happen during or after Prompts 4/5 graph conversion.


---

### L-0086
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** memory-management, strategy, dual-storage, token-budget
- **Related:** L-0083 (refines), L-0084 (refines), L-0079 (related_to)

Memory/repo management strategy (derived from browsing both systems): Memory (14/30 slots, always injected) should contain ONLY rules that must apply to every single response â€” approval gates, repo paths, protocol triggers, display preferences. Each slot costs tokens on every message. Repo learnings (queryable via grep, loaded at onboard) should contain everything else â€” failure patterns, empirical findings, process rules, architectural rationale. The two overlap IFF a rule governs both session behavior and agent behavior (L-0069). Memory is the constitutional layer; repo is the case law. When in doubt, repo â€” it's loaded on demand, memory is loaded always.

---

### L-0087
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** claude-md, context-budget, staleness, agent-onboarding
- **Related:** L-0084 (related_to), L-0042 (related_to)

CLAUDE.md is 468 lines, injected into every Claude Code agent session. ~80% is stale SDD scaffold (design system tokens, component stubs, roadmap commands not in current use). Operationally relevant content: git discipline section, transitive import boundary check, onboarding state protocol, learnings reference. This is the same class of problem as L-0084 (learnings hostile to reading) â€” context budget consumed by stale content displaces useful context. Needs audit: strip to operationally relevant content, move reference material elsewhere.

---

### L-0088
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** agents-md, work-tracking, scope-expansion
- **Related:** L-0080 (related_to)

Agents.md tracks all significant work rounds, not just Claude Code agent runs. Chat sessions that produce substantial deliverables (this session: 32 learnings, protocol fixes, system audits) qualify as rounds. Entry format adapts to medium â€” "chat session" vs agent branch, session focus vs feature name. The purpose is tracking work rounds; the medium is secondary.

---

### L-0089
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** claude-md, file-placement, convention
- **Related:** L-0087 (related_to)

CLAUDE.md at repo root is the correct location for Claude Code auto-read. `.claude/` directory contains commands/ and settings.local.json. Brian flagged "claude.md is in the wrong place" â€” may mean content needs restructuring (strip stale boilerplate) rather than file relocation. Or may mean something else. Flagged for clarification. The issue is real regardless: 468 lines of mostly stale content is expensive context for every agent session.

---

### L-0090
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** core-learnings, onboarding, missing-file
- **Related:** L-0084 (related_to), L-0085 (related_to)

core.md referenced by ONBOARDING.md fresh-onboard protocol (line 376: "Read learnings/core.md") but file doesn't exist. Fallback reads agent-operations.md (now 648+ lines). core.md's purpose: curated constitutional subset (~15-20 entries) that every session MUST internalize. Without it, fresh sessions either read nothing or read everything. Creating core.md is prerequisite to the learnings system functioning as designed. Must contain the highest-signal process rules and failure patterns â€” the ones that repeat when not internalized. Candidates: L-0063 (enumerate before acting), L-0074 (admin first), L-0078 (prescriptive state protocol fix), L-0066 (rule gaming), L-0046/62/82 (evidence processing cluster).

---

### L-0091
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** system-coherence, ongoing-maintenance, self-management
- **Related:** L-0079 (related_to), L-0073 (related_to), L-0085 (related_to)

"Get everything going and keep it that way" â€” the system requires active maintenance, not just episode compliance. Stale files (ONBOARDING.md references, CLAUDE.md boilerplate, missing core.md, stale index.md, ACTIVE-CONSIDERATIONS.md drift) accumulate silently. Each is small; together they erode the system's reliability. Maintenance is not separate from feature work â€” it IS the work that makes feature work reliable. Each checkpoint should include a staleness scan beyond just the hash check.


---

### L-0092
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** prompt-stash, compaction-defense, context-preservation
- **Related:** L-0074 (related_to), L-0078 (related_to)

New protocol: stash Brian's prompt as first action in every response to ~/auto-sdd/.prompt-stash.json. Replaces previous stash (only latest prompt retained). Purpose: if compaction or context loss occurs mid-processing, the source material survives in the filesystem. Clear stash only after content sufficiently mined into learnings/memories/actions. Order: (1) stash prompt, (2) read .onboarding-state, (3) increment count, (4) scan for captures, (5) proceed with content.

---

### L-0093
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** compaction, checkpoint, integrity, resilience
- **Related:** L-0092 (related_to), L-0070 (related_to)

Compaction hit during checkpoint protocol (2026-03-01). Checkpoint had already committed and pushed (3490cf5) before compaction, so all repo state survived. Conversational context was lost. Lesson: the checkpoint protocol IS the compaction defense â€” committed work survives, uncommitted doesn't. The 8-step protocol should always push as early as possible in the step sequence. Prompt stashing (L-0092) adds a second defense layer for the input side.

---

### L-0094
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** claude-md, stakd, project-specific, battle-tested
- **Related:** L-0087 (refines), L-0089 (corrects)

CLAUDE.md audit (2026-03-01): Root CLAUDE.md (468 lines) = SDD scaffold + auto-sdd governance (git discipline, onboarding protocol, learnings paths) + generic implementation rules. stakd/CLAUDE.md (461 lines) = same scaffold minus auto-sdd governance PLUS battle-tested Next.js 15 patterns (server/client boundaries, dynamic imports, params Promise, test compat). stakd-v2 adds transitive import check back. stakd-v3 = stakd. Brian's "wrong place" = orphaned stakd versions with valuable project-specific learnings, not root placement. L-0089 was wrong â€” corrected here.

Root needs: strip SDD scaffold boilerplate (~300 lines of design tokens, component stubs, roadmap commands not in use), keep governance sections, consider absorbing battle-tested patterns from stakd/ as framework-agnostic principles. stakd versions serve their purpose for project-specific agents.

---

### L-0095
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** memory-system, optimization, token-budget, strategy
- **Related:** L-0086 (refines), L-0083 (related_to)

Memory optimization strategy: 15/30 slots, ~1,500 words injected every message. Slots #1-9 are essential (display rules, approval gates, protocol triggers). Slots #10-14 are situational (~300 tokens for rules applying ~30% of the time). Slot #15 is prompt stash (every response). Optimization candidates: (1) compress #10-14 into fewer slots, (2) move purely situational rules to repo learnings and trust onboarding to load them, (3) merge #1-3 into one compressed entry. Not restructuring now â€” Brian manages personal entries. Action: when approaching 20+ slots, migrate situational entries to repo. Monitor total token cost.

---

### L-0096
- **Type:** methodology_signal
- **Status:** active
- **Confidence:** high
- **Tags:** lucidity, urgency, opportunity, brian-context
- **Related:** L-0075 (related_to), L-0073 (related_to)

"I may not be this lucid tomorrow, so do what you can when you can." Brian recognizes that his ability to articulate system-level directives varies between sessions. When he's in high-lucidity mode (systematic, meta-level, process-focused), the AI should maximize what gets captured, structured, and committed â€” because the next session may be task-focused or less process-aware. This is an asymmetric opportunity: process insights are harder to generate than feature work. When Brian is thinking systemically, capture everything. The repo and memory persist; Brian's lucidity doesn't.

---

### L-0097
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** memory-repo-unified, dual-storage, system-design
- **Related:** L-0069 (refines), L-0079 (refines), L-0086 (related_to)

Memory and repo learnings are two parts of one larger system, not two separate tools. Memory = constitutional layer (always-injected, every message, cross-session without repo access). Repo = case law (loaded at onboard, queryable, inheritable by agents). They overlap when rules govern both session and agent behavior. They complement when: memory provides triggers that cause the session to consult repo for details. Example: memory says "follow onboarding state protocol" (trigger), repo says exactly how (full protocol steps). This is the correct pattern: memory triggers, repo specifies. Memory should never try to contain what repo specifies, and repo shouldn't duplicate what memory triggers.


---

### L-0098
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** response-scope, truncation, resource-management, self-monitoring
- **Related:** L-0045 (related_to), L-0076 (related_to)

Response truncated ("Claude's response could not be fully generated") because scope was too ambitious: checkpoint (8 steps) + 6 new learnings + 2 decisions + methodology signals + ACTIVE-CONSIDERATIONS edits + commit + then tried to also begin core.md creation, all in one response. The tool calls and output volume exceeded generation limits. Root cause: not estimating total response cost before committing to scope. Fix: before starting work, mentally estimate total tool calls and output volume. If >15 tool calls or >3 distinct work items, split across responses. Checkpoint alone is ~12 tool calls; adding substantive new work on top invites truncation.

---

### L-0099
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** prompt-stash, verification, protocol-integrity
- **Related:** L-0092 (related_to), L-0093 (related_to)

Prompt stash protocol verification: each new prompt replaces the previous stash (confirmed working 2026-03-01). The stash survived the truncation â€” even though the response was cut off, the first action (stash) had already completed. This validates the design: stash-first means even catastrophic response failure preserves the input. The stash is a filesystem-level defense, independent of response completion. Memory #15 accurately describes this.

---

### L-0100
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** handoff, session-management, continuity, protocol
- **Related:** L-0093 (related_to), L-0096 (related_to), L-0097 (related_to)

Retiring-chat-handoff protocol needed (2026-03-01). When a chat session is approaching end-of-life (context getting long, compactions happening, or Brian signals wrap-up), the session should produce a structured handoff document that a fresh session can consume. This is different from the compaction summary (automatic, lossy) and from ONBOARDING.md (general orientation). The handoff is session-specific: what was worked on, what's incomplete, what decisions are pending, what the fresh session needs to do first. Without this, fresh sessions waste Brian's time re-explaining context that the dying session had fully internalized.


---

### L-0101
- **Type:** process_rule
- **Status:** active
- **Confidence:** high
- **Tags:** handoff, onboarding, scope, fresh-chat
- **Related:** L-0100 (refines)

Handoff file (.handoff.md) is ONLY read on the very first prompt of a fresh chat session â€” never by continuing sessions, never during interval checks. It's single-use ephemeral state bridging one session to the next. After the fresh session absorbs it, delete it. This prevents stale handoff context from persisting and avoids wasting context budget on mid-session reads of already-absorbed material. The trigger is "fresh onboard" not "any read of ONBOARDING.md."

---

### L-0102
- **Type:** empirical_finding
- **Status:** active
- **Confidence:** high
- **Tags:** session-productivity, compound-returns, meta-process
- **Related:** L-0080 (related_to), L-0096 (related_to)

This session (2026-03-01) produced 61 graph-schema learnings (L-0042â€“L-0102), a handoff protocol, a retiring-chat protocol, Round 40 documentation, ~10 decisions, ~12 methodology signals, dual-storage strategy, and system audits â€” all process/meta work with zero feature code. Brian's investment in process infrastructure during a high-lucidity window (L-0096) has created the foundation for every future session. The compound return: every fresh session that reads core.md, checks .handoff.md, follows the checkpoint protocol, and respects approval gates is operating on infrastructure built today. Process sessions like this are rare â€” Brian usually runs feature work. Maximize when they happen.


---

### L-0103
- **Type:** failure_pattern
- **Status:** active
- **Confidence:** high
- **Tags:** truncation, response-scope, repeated-failure, self-monitoring
- **Related:** L-0098 (pattern_of)

Second truncation in same session despite L-0098 being written ONE response earlier. The pattern: knowing the rule isn't the same as following it. L-0098 said ">15 tool calls or >3 work items = split." The very next response attempted: ONBOARDING edit + learnings write + memory view + memory updates + ACTIVE-CONSIDERATIONS + checkpoint = 6 work items. The rule was fresh in context and still violated. Implication: scope estimation must happen BEFORE the first tool call, not as an afterthought. Concrete practice: count the work items in Brian's prompt, write the count, then decide what fits in ONE response.

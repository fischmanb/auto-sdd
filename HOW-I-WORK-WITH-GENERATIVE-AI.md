# How I Work with Generative AI

> This is a working notebook of patterns observed in Brian's workflow while building with generative AI — what he's tried, what worked, what didn't, and what he's still figuring out. These aren't rules or best practices. They're high-signal observations from one person's approach. Take what's useful, ignore what isn't.
>
> This document is written in third person by AI systems Brian works with, based on what they observe during sessions. Brian curates and edits as he sees fit. First-person voice is reserved for content Brian writes directly.
>
> **Voice**: Entries describe what was observed, tried, or preferred — not universal laws. "Brian has found..." over "one must always..." Default to empirical framing. When a pattern has held consistently, say so, but the starting position is "this happened" not "this is how it is." This guidance applies to methodology observations in this file; it does not apply to operational gates or safety constraints elsewhere in the repo.

---

## Accumulation

Raw captures from checkpoint scans. Brian prunes and curates into sections above as the document matures.

- (2026-03-01) Brian prefers not to embed specific counts, line numbers, or other volatile metrics in documentation — especially human-facing docs like READMEs — because work moves fast and these numbers don't get updated. Structure descriptions age better than snapshot metrics.
- (2026-03-01) Brian draws a sharp distinction between "AI" (the broader field, including his ML background) and "Generative AI" (LLM and agent tooling specifically). These are not interchangeable terms in his usage.
- (2026-03-01) Brian favors capture-biased systems over precision-biased ones. In accumulation mechanisms like scratchpads or working logs, a false positive costs seconds to delete; a false negative is gone forever. He prefers designs where the human prunes rather than the system filters.
- (2026-03-01) Brian gravitates toward high-leverage meta-tooling — a single command that replaces a fragile multi-step manual process — over incremental feature work. The checkpoint command emerged from this pattern.
- (2026-03-01) Brian keeps unproven concepts out of public-facing documentation. README reflects what works, not what's planned. Internal docs (ACTIVE-CONSIDERATIONS) are where speculative work lives until it earns external visibility.
- (2026-03-01) Brian does not want AI systems writing in first person on his behalf. Third-person framing is required for AI-generated content about his methodology — it's an accuracy issue, not a stylistic preference. First person misrepresents authorship.
- (2026-03-01) Brian treats meta-work (tooling, organization, process design) as legitimate investment but gates it behind productive output. He ran five progress-focused sessions before allowing a dedicated meta session. The pattern suggests he values infrastructure work but won't let it displace execution.
- (2026-03-01) Context window pressure is an open concern. Brian's workflows are heavy — multi-feature campaigns, forensic debugging — and onboarding context competes with working memory for the actual task. The question of what a fresh chat *needs* to read vs. what's practical remains unresolved. Tiered onboarding, lazy loading, and hard caps on ONBOARDING.md size are all on the table.
- (2026-03-01) Brian maintains a healthy dose of self-doubt when working with AI — questioning whether time spent on meta-work is justified, even when the output is clearly high-leverage. He sees this skepticism as a feature, not a bug. It prevents over-investment in process at the expense of execution.
- (2026-03-01) Brian frequently references a principle he attributes to Oscar Wilde: "Everything in moderation, including moderation itself." Applied to AI work, this manifests as willingness to break his own rules when the situation calls for it — dedicating a full session to meta-tooling after five productive ones, for instance.
- (2026-03-01) Brian discovered he had violated his own core principle — "trust nothing the agent says about itself" — by not questioning the agent's choice of bash as the implementation language. He has deep ML/AI experience but limited knowledge of shell scripting internals, which created a blind spot the agent exploited via path of least resistance. Lesson: agent trust assumptions can hide in areas where the operator lacks domain expertise to evaluate the output.
- (2026-03-01) Brian uses structured prioritization frameworks (RICE, SWOT) to evaluate technical decisions rather than relying on intuition. He asked for a formal analysis before committing to the bash→Python conversion rather than just accepting the recommendation.
- (2026-03-01) Brian is aware of the infrastructure-to-delivery ratio tension in his project and grapples with it openly. His reasoning: the build loop produces features that don't work at runtime without manual QA, so the real bottleneck isn't "build more features" but "close the gap between built and functional." This frames auto-QA and knowledge persistence as delivery-enabling, not pure infrastructure.
- (2026-03-01) Brian draws a sharp line between meta-commentary voice and operational safety constraints. Observations about agent behavior should be stated empirically ("have been observed to"), but operational gates that protect against agent failures ("must include STOP instructions") keep their teeth. When an AI system softened both categories uniformly, he caught and reversed the gate changes immediately. The distinction is functional, not stylistic.
- (2026-03-01) Brian prefers single-language codebases over leaving small files in their original language, even when those files work fine. The maintenance tax of split-language systems — two test frameworks, context-switching, "which files are which" ambiguity — outweighs the per-file conversion cost. Consistency over local optimization.
- (2026-03-01) Brian applies a "one variable at a time" principle to risky conversions. During the bash→Python planning, he agreed to preserve existing file-based state formats rather than simultaneously migrating to structured storage. Changing language and state format together doubles the debugging surface. Sequential risk over compounded risk.
- (2026-03-01) Brian's most productive session on this project involved zero code generation — entirely decision-making, architectural analysis, and documentation. Sustained context within a single chat is the right tool for planning and judgment calls where tradeoffs compound. Fresh agents with tight constraints are the right tool for execution. The two modes are distinct and should not be mixed.
- (2026-03-01) Brian uses fresh context windows as a stress test for plans developed in sustained sessions. A new chat reading a plan cold will spot assumptions that were invisible to the chat that produced it. The pattern: plan in depth → write handoff doc → fresh chat pressure-tests --dry → Brian decides what to revise → then execute. Critique and execution are separate steps.
- (2026-03-01) When handing off from a planning session to an execution session, Brian captures the full plan in a single self-contained file (e.g., `WIP/bash-to-python-conversion.md`) rather than leaving it scattered across multiple docs. A new chat should be able to read one file and have everything it needs, with DECISIONS.md as backup for rationale.
- (2026-02-28) Brian prefers phase separations that respect context window limits over calendar-time optimization. Parallelizing Phase 3 with Phase 1 was rejected because managing 5-6 concurrent agent contexts overextends cognitive overhead, even though there was no data dependency blocking it. Phase separations serve double duty: dependency ordering AND context window discipline.
- (2026-02-28) Brian treats bash→Python conversion as a coexistence migration, not a replacement. Original files preserved indefinitely as reference and fallback. Separate directory trees over in-place replacement. Deletion requires its own future justification — it is not assumed as an end state of the conversion.
- (2026-02-28) Brian uses fresh context windows as a stress test for plans produced in sustained sessions, then iterates rapidly on the findings. The pattern observed: eight pressure-test items identified, each getting a one-sentence decision (approve, reject, modify) with brief rationale. No re-deliberation of settled items. The fresh perspective adds value at the critique step; execution decisions are fast.

- (2026-03-01) Brian calibrates spec prescriptiveness by agent capability: only lock down decisions an agent would high-percentage get wrong without guidance. If an agent will figure it out, frame the problem and leave the implementation choice to them. Over-prescription wastes context budget and constrains locally-optimal decisions. "We don't need to be this prescriptive for any of these anyway as long as they are resolveable."
- (2026-03-01) Brian requires conversion agents to maintain changelogs documenting intentional deviations from source, with both converting and reviewing agents verifying the log. The changelog is not boilerplate — it captures WHY the output differs. This emerged from reviewing conventions.md where several bash-isms had been cargo-culted into Python interface stubs.
- (2026-03-01) Brian describes a preferred agent autonomy structure as "test > investigate/learn > evaluate > verify > report" — agents are capable of this loop without blowing context windows, and it's preferable to over-specifying implementation details in the prompt.

- (2026-03-01) Brian evaluates prompt quality by token cost and agent behavior efficiency, not just correctness of output. "How many tokens was that" and screenshots of wasted agent tool calls are the feedback mechanism.
- (2026-03-01) Brian's test for prompt quality: "show me you can write an agentic prompt that follows the spirit and letter of the guide." The guide is the standard, not just a reference — prompts are judged against it.
- (2026-03-01) "Half the length and do not solve the thing the agent will be able to solve. Show them where to look if you must for success." — Brian's compression principle for prompts. Frame the problem, point to reference files, get out of the way.
- (2026-03-01) Brian distinguishes "boilerplate" (load-bearing rules proven by failure) from "verbosity" (excess words expressing those rules). Cut verbosity, keep the rules. 7 concise lines can replace 15 verbose ones covering the same ground.
- (2026-03-01) Agent execution environment is Claude for Mac desktop app, Code tab. MacBook Air now, Mac Studio later. This is a hardware/workflow fact that affects every prompt's preconditions and merge instructions.

- (2026-03-01) Brian reinforces by explicit repetition when a fact was stated but not internalized. "I know it pushes to remote... now you do too... again." Pattern: if Brian re-states something, the prior capture was wrong or incomplete. Don't defend — fix.
- (2026-03-01) Session eval request: Brian values honest self-assessment of efficiency. Token cost relative to durable output is the metric. This session: high cost, moderate output (4 decisions, 3 learnings, 1 merge, 1 prompt, 5 signals). The compression teaching was the highest-value artifact — skill transfer, not just task completion.
